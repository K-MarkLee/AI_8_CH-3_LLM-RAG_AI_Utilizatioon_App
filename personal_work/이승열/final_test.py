import streamlit as st
import logging
import os
import pickle
from gtts import gTTS

from dotenv import load_dotenv
import base64
import tempfile


from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory


load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    raise EnvironmentError("Error: OpenAI_API_KEY is not set. Please configure it in your environment.")
os.environ["OpenAI_API_KEY"] = api_key





# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



# ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
path = "food_db/"
vectorstore = None


# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_vectorstore(path):
    try:
        return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
        
        
def get_conversation_chain(vectorstore, prompt):
    
    """ëŒ€í™” ì²´ì¸ ìƒì„±"""
    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)


    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        # ë¦¬íŠ¸ë¦¬ë²„ ì—°ê²°í•˜ê¸° (ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ 5ê°œ ì¶”ì¶œí•˜ê¸°)
        retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
        memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer"),
        combine_docs_chain_kwargs={"prompt": prompt},
        get_chat_history=lambda h: h,
        return_source_documents=True
    )



# í”„ë¡¬í”„íŠ¸ ë¡œë“œ
def load_prompts(path, system_files):
    system_message = []
    for txt in system_files:
        try:
            with open(os.path.join(path, txt), "r", encoding="UTF-8") as f:
                content = f.read().replace("\\n", "\n")
                system_message.append(("system", content))
        except FileNotFoundError:
            print(f"Warning: Prompt file '{txt}' not found.")
        except Exception as e:
            print(f"Error loading prompt '{txt}': {e}")
    
    system_message.append(("user", "data : {data}\\n\\nQuestion: {question}"))

    return system_message

    # í˜¸ì¶œ
system_message = load_prompts("Prompts/", ["Require_decide.txt", "Food_recipe.txt", "Food_recommend.txt"])
prompt = ChatPromptTemplate.from_messages(system_message)






def autoplay_audio(audio_content, autoplay=True):
    """ìŒì„± ì¬ìƒì„ ìœ„í•œ HTML ì»´í¬ë„ŒíŠ¸ ìƒì„± (1.5ë°°ì†)"""
    b64 = base64.b64encode(audio_content).decode()
    md = f"""
        <audio {' autoplay' if autoplay else ''} controls>
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
        </audio>
        <script>
            document.addEventListener('DOMContentLoaded', function() {{
                const audioElements = document.getElementsByTagName('audio');
                for(let audio of audioElements) {{
                    audio.playbackRate = 1.5;
                }}
            }});
        </script>
        """
    return st.markdown(md, unsafe_allow_html=True)



def text_to_speech(text, lang='ko'):
    """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
            tts = gTTS(text=text, lang=lang)
            tts.save(fp.name)
            with open(fp.name, 'rb') as audio_file:
                audio_bytes = audio_file.read()
            os.unlink(fp.name)
            return audio_bytes
    except Exception as e:
        logger.error(f"ìŒì„± ë³€í™˜ ì˜¤ë¥˜: {e}")
        return None



def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "initialized" not in st.session_state:
        st.session_state.initialized = True
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {
                "role": "assistant", 
                "content": "ì•ˆë…•í•˜ì„¸ìš”! ìš”ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì–´ë–¤ ìš”ë¦¬ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                "audio": None
            }
        ]
    if "voice_enabled" not in st.session_state:
        st.session_state.voice_enabled = True



def validate_api_key(api_key):
    """OpenAI API í‚¤ í˜•ì‹ ê²€ì¦"""
    return api_key and len(api_key) > 20



def main():
    try:
        # í˜ì´ì§€ ì„¤ì •
        st.set_page_config(
            page_title="ìš”ë¦¬ ë„ìš°ë¯¸",
            page_icon="ğŸ³",
            layout
            ="wide"
        )
        st.title("ìš”ë¦¬ ë„ìš°ë¯¸ ğŸ³")
            

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        initialize_session_state()
        global vectorstore
        if vectorstore is None:
            vectorstore = load_vectorstore(path)
            



        # ìŒì„± ì¶œë ¥ í† ê¸€ ë° API í‚¤ ì…ë ¥ì„ í—¤ë” ì•„ë˜ì— ë°°ì¹˜
        col1, col2 = st.columns([1, 2])
        with col1:
            st.session_state.voice_enabled = st.toggle("ìŒì„± ì¶œë ¥ í™œì„±í™”", value=st.session_state.voice_enabled)


        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        chat_container = st.container()
        with chat_container:
            for i, message in enumerate(st.session_state.messages):
                with st.chat_message(message["role"]):
                    st.write(message["content"])
                    if message["role"] == "assistant" and st.session_state.voice_enabled:
                        if message.get("audio") is None and message["content"]:
                            audio_bytes = text_to_speech(message["content"])
                            if audio_bytes:
                                message["audio"] = audio_bytes

                        if message.get("audio"):
                            cols = st.columns([1, 4])
                            with cols[0]:
                                if st.button("ğŸ”Š ì¬ìƒ", key=f"play_message_{i}"):
                                    autoplay_audio(message["audio"])
                            with cols[1]:
                                autoplay_audio(message["audio"], autoplay=False)

        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.session_state.messages.append({"role": "user", "content": query, "audio": None})
            
            with st.chat_message("user"):
                st.write(query)

            if not validate_api_key(api_key):
                response = "OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
                st.warning(response)
                
                if st.session_state.voice_enabled:
                    audio_bytes = text_to_speech(response)
                else:
                    audio_bytes = None
                    
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "audio": audio_bytes
                })
                
                if audio_bytes:
                    autoplay_audio(audio_bytes)
                
                st.stop()

            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                    try:
                        # food_DB í´ë”ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ì§ì ‘ ë¡œë“œ
                        if not st.session_state.conversation:
                            vectorstore_path = path

                            st.session_state.conversation = get_conversation_chain(vectorstore, api_key)


                        result = st.session_state.conversation({"question": query})
                        response = result['answer']
                        source_documents = result.get('source_documents', [])

                        st.write(response)

                        if st.session_state.voice_enabled:
                            audio_bytes = text_to_speech(response)
                            if audio_bytes:
                                autoplay_audio(audio_bytes)
                        else:
                            audio_bytes = None

                        if source_documents:
                            with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                                for i, doc in enumerate(source_documents[:3], 1):
                                    st.markdown(f"**ì°¸ê³  {i}:** {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')}")
                                    st.markdown(f"```\n{doc.page_content[:200]}...\n```")

                        st.session_state.messages.append({
                            "role": "assistant", 
                            "content": response,
                            "audio": audio_bytes
                        })

                    except Exception as e:
                        error_message = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                        st.error(error_message)
                        
                        if st.session_state.voice_enabled:
                            audio_bytes = text_to_speech(error_message)
                        else:
                            audio_bytes = None
                            
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": error_message,
                            "audio": audio_bytes
                        })
                        
                        if audio_bytes:
                            autoplay_audio(audio_bytes)
                            
                        logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")

    except Exception as e:
        logger.error(f"ì•± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error("ì•± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ì„ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == '__main__':
    main()